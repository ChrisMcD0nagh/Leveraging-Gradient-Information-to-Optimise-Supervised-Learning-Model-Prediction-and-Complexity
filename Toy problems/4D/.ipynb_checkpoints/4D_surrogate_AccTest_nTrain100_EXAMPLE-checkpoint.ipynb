{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4D GARCH surrogate training code\n",
    "\n",
    "This notebook provides an example of the code used to train and evaluate the accuracy of surrogate models for the 4D toy problem discussed in Chapter 3 Section 3.3. \n",
    "\n",
    "Original experiments were run in Google Colab using TPUs. \n",
    "\n",
    "This code is replicated with varying training sizes to produce the full result set. Full code and models are available [here](https://drive.google.com/drive/folders/1J7srZbZPS6UhE43GFXP3Gkd3TmEvT-6f)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3373,
     "status": "ok",
     "timestamp": 1629834055529,
     "user": {
      "displayName": "Chris Mcdonagh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhF-UpNVeUXFvpwslXd-aD8GJZPFHZo2-yZhUuE-A=s64",
      "userId": "16046156616630541706"
     },
     "user_tz": -60
    },
    "id": "N8Ezc5LO_QDV"
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda, dot, concatenate, PReLU, Dropout, advanced_activations\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import adam_v2\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import concurrent.futures\n",
    "from time import time\n",
    "import gc\n",
    "from scipy import stats\n",
    "from datetime import *\n",
    "from time import time as time1\n",
    "import os\n",
    "import subprocess\n",
    "#from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1629834055530,
     "user": {
      "displayName": "Chris Mcdonagh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhF-UpNVeUXFvpwslXd-aD8GJZPFHZo2-yZhUuE-A=s64",
      "userId": "16046156616630541706"
     },
     "user_tz": -60
    },
    "id": "wYDJk9vd_mCI"
   },
   "outputs": [],
   "source": [
    "# Suppress retracing  and auograph error\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1629834055531,
     "user": {
      "displayName": "Chris Mcdonagh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhF-UpNVeUXFvpwslXd-aD8GJZPFHZo2-yZhUuE-A=s64",
      "userId": "16046156616630541706"
     },
     "user_tz": -60
    },
    "id": "fMvXCMPB_wL8"
   },
   "outputs": [],
   "source": [
    "nInputDim = 4 # Latent parameters\n",
    "nOutputDim = 1 # Vol output\n",
    "\n",
    "T = 5 # Time window forecast\n",
    "forecast_window = range(1, T + 1)\n",
    "base_hidden_size = 4\n",
    "\n",
    "nBatchSize = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "\n",
    "Initial (t+1) surrogate takes only latent parameters $\\theta$ as an input while subsequent surrogates take latent parameters and previous output as described in Chapter 2 Section 2.1.1. This allows the surrogate model to generate volatility predictions for t+h for h 2-5.\n",
    "\n",
    "Surrogate models use architecture with 4 hidden layers, each with $\\eta$ * 4 nodes where $\\eta$ refers to model complexity as described in Chapter 3 Section 3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1629834055532,
     "user": {
      "displayName": "Chris Mcdonagh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhF-UpNVeUXFvpwslXd-aD8GJZPFHZo2-yZhUuE-A=s64",
      "userId": "16046156616630541706"
     },
     "user_tz": -60
    },
    "id": "bSZTZ6lAkrU1"
   },
   "outputs": [],
   "source": [
    "def make_model_t1(complexity=1, lr=0.01, g_w=0.5):\n",
    "    \"\"\"\"Makes surrogates for for t+1 vol pred.\n",
    "    The weight assigned to the loss function that fits gradients can be set with g_w.\n",
    "    Model complexity integer and determines the number of nodes in each \n",
    "    hidden layer.\n",
    "    \"\"\"\n",
    "    theta = Input(shape=nInputDim)\n",
    "    prev_vol = Input(shape=1)\n",
    "    prev_return = Input(shape=1)\n",
    "    concat = concatenate([theta, prev_vol, prev_return])\n",
    "    h1 = Dense(complexity * base_hidden_size, activation=\"tanh\")(concat)\n",
    "    h2 = Dense(complexity * base_hidden_size, activation=\"tanh\")(h1)\n",
    "    h3 = Dense(complexity * base_hidden_size, activation=\"tanh\")(h2)\n",
    "    h4 = Dense(complexity * base_hidden_size, activation=\"tanh\")(h3)\n",
    "    out = Dense(nOutputDim, activation='softplus')(h4)\n",
    "    \n",
    "    grad = Lambda(lambda x: K.gradients(x[0], [x[1]])[0], output_shape=nInputDim)([out, theta])\n",
    "    model = Model(inputs=[theta, prev_vol, prev_return], outputs=[out, grad])\n",
    "    opt = adam_v2.Adam(learning_rate=lr)\n",
    "    model.compile(loss=['mse', 'mse'], optimizer=opt, metrics=['accuracy'], loss_weights=[1-g_w, g_w])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1629834055533,
     "user": {
      "displayName": "Chris Mcdonagh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhF-UpNVeUXFvpwslXd-aD8GJZPFHZo2-yZhUuE-A=s64",
      "userId": "16046156616630541706"
     },
     "user_tz": -60
    },
    "id": "X1pbTZJeW8yb"
   },
   "outputs": [],
   "source": [
    "def make_model_tsub(complexity=1, lr=0.01, g_w=0.5):\n",
    "    \"\"\"\"Makes surrogates for for t+h vol pred where 2 <= h <= T.\n",
    "    Note that for t+h preds resid_t is not needed (refer to ARCH docs for more detail.).\n",
    "    The weight assigned to the loss function that fits gradients can be set with g_w.\n",
    "    Model complexity integer and determines the number of nodes in each \n",
    "    hidden layer.\n",
    "    \"\"\"\n",
    "    theta = Input(shape=nInputDim)\n",
    "    prev_vol = Input(shape=1)\n",
    "    concat = concatenate([theta, prev_vol])\n",
    "    h1 = Dense(complexity * base_hidden_size, activation=\"tanh\")(concat)\n",
    "    h2 = Dense(complexity * base_hidden_size, activation=\"tanh\")(h1)\n",
    "    h3 = Dense(complexity * base_hidden_size, activation=\"tanh\")(h2)\n",
    "    h4 = Dense(complexity * base_hidden_size, activation=\"tanh\")(h3)\n",
    "    out = Dense(nOutputDim, activation='softplus')(h4)\n",
    "    \n",
    "    grad = Lambda(lambda x: K.gradients(x[0], [x[1]])[0], output_shape=nInputDim)([out, theta])\n",
    "    model = Model(inputs=[theta, prev_vol], outputs=[out, grad])\n",
    "    opt = adam_v2.Adam(learning_rate=lr)\n",
    "    model.compile(loss=['mse', 'mse'], optimizer=opt, metrics=['accuracy'], loss_weights=[1-g_w, g_w])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1629834055533,
     "user": {
      "displayName": "Chris Mcdonagh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhF-UpNVeUXFvpwslXd-aD8GJZPFHZo2-yZhUuE-A=s64",
      "userId": "16046156616630541706"
     },
     "user_tz": -60
    },
    "id": "AAZClQdaZC5c"
   },
   "outputs": [],
   "source": [
    "def build_models(lr=0.01):\n",
    "    \"\"\" Store models in nested dictionary of the form [time_step][complexity].\n",
    "    \"\"\"\n",
    "    \n",
    "    models_std = defaultdict(dict)\n",
    "    models_grad = defaultdict(dict)\n",
    "    \n",
    "    for t in forecast_window:\n",
    "        for m in complexity_range:\n",
    "            if t == 1:\n",
    "                models_std[str(t)][str(m)] = make_model_t1(lr=lr, complexity=m, g_w=0)\n",
    "                models_grad[str(t)][str(m)] = make_model_t1(lr=lr, complexity=m)\n",
    "            else:\n",
    "                models_std[str(t)][str(m)] = make_model_tsub(lr=lr, complexity=m, g_w=0)\n",
    "                models_grad[str(t)][str(m)] = make_model_tsub(lr=lr, complexity=m)\n",
    "        \n",
    "    return models_std, models_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1629834055534,
     "user": {
      "displayName": "Chris Mcdonagh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhF-UpNVeUXFvpwslXd-aD8GJZPFHZo2-yZhUuE-A=s64",
      "userId": "16046156616630541706"
     },
     "user_tz": -60
    },
    "id": "CEzSP0bkZFWZ"
   },
   "outputs": [],
   "source": [
    "def fit_model(model, params, prev_vol, prev_ret, prev_vol_sub, y, y_grad, t, complexity, foldername):\n",
    "    \"\"\" Fits the NN models. Note that prev_vol is vol at t=0 (inital vol). Subsequent \n",
    "    vol predictions use prior (true) vol outputs.\n",
    "    \"\"\"\n",
    "    filename = f'{foldername}/GARCH_example_N{nTrain}_E{nEpochs}_c{complexity}_t{t}.h5'\n",
    "    if t == 1:\n",
    "        model.fit([params, prev_vol, prev_ret], [y, y_grad], batch_size=nBatchSize, epochs=nEpochs, verbose=0,\n",
    "            use_multiprocessing=True, callbacks=[LearningRateScheduler(lr_time_based_decay)])\n",
    "    else:\n",
    "        model.fit([params, prev_vol_sub], [y, y_grad], batch_size=nBatchSize, epochs=nEpochs, verbose=0,\n",
    "            use_multiprocessing=True, callbacks=[LearningRateScheduler(lr_time_based_decay)])\n",
    "    \n",
    "    model.save(filepath=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1629834055535,
     "user": {
      "displayName": "Chris Mcdonagh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhF-UpNVeUXFvpwslXd-aD8GJZPFHZo2-yZhUuE-A=s64",
      "userId": "16046156616630541706"
     },
     "user_tz": -60
    },
    "id": "xXCNw-1UZL8T"
   },
   "outputs": [],
   "source": [
    "def train_models(curr_data, models_std, models_grad, foldername):\n",
    "    \n",
    "    simInData_latent = curr_data['simInData_latent']\n",
    "    simInData_vol = curr_data['simInData_vol']\n",
    "    simInData_ret = curr_data['simInData_ret']\n",
    "    simOutData = curr_data['simOutData']\n",
    "    simOutData_grad = curr_data['simOutData_grad']\n",
    "\n",
    "    resid = simInData_ret.iloc[:, 0] - simInData_latent.iloc[:, 0]\n",
    "    \n",
    "    no_models = len(complexity_range)\n",
    "\n",
    "    foldername_grad = foldername + '_grad'\n",
    "    t_curr = time1()\n",
    "    for t in forecast_window:\n",
    "        t_min = max(t-1, 1) # To avoid error when t = 1.\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=no_models) as executor:\n",
    "            future1 = {executor.submit(fit_model, \n",
    "                                        m, \n",
    "                                        simInData_latent.values, \n",
    "                                        simInData_vol.values,\n",
    "                                        resid.values, \n",
    "                                        simOutData.loc[:, t_min].values,  # Used as input for t+h.\n",
    "                                        simOutData.loc[:, t].values, \n",
    "                                        simOutData_grad.loc[:, str(t)].values,\n",
    "                                        t,\n",
    "                                        complexity,\n",
    "                                        foldername) \n",
    "                    for complexity, m in models_std[str(t)].items()}\n",
    "\n",
    "            future2 = {executor.submit(fit_model, \n",
    "                                        m, \n",
    "                                        simInData_latent.values, \n",
    "                                        simInData_vol.values,\n",
    "                                        resid.values, \n",
    "                                        simOutData.loc[:, t_min].values,  # Used as input for t+h.\n",
    "                                        simOutData.loc[:, t].values, \n",
    "                                        simOutData_grad.loc[:, str(t)].values,\n",
    "                                        t,\n",
    "                                        complexity,\n",
    "                                        foldername_grad) \n",
    "                    for complexity, m in models_grad[str(t)].items()}\n",
    "            \n",
    "        print(f'Completed timestep {t} in {time1() - t_curr:.02f}s...')\n",
    "        t_curr = time1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1629834055536,
     "user": {
      "displayName": "Chris Mcdonagh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhF-UpNVeUXFvpwslXd-aD8GJZPFHZo2-yZhUuE-A=s64",
      "userId": "16046156616630541706"
     },
     "user_tz": -60
    },
    "id": "VurmpHlVZO9b"
   },
   "outputs": [],
   "source": [
    "def load_models(N, foldername):\n",
    "    std_models = defaultdict(dict)\n",
    "    grad_models = defaultdict(dict)\n",
    "\n",
    "    grad_foldername = foldername + '_grad' \n",
    "    \n",
    "    for t in forecast_window:\n",
    "        for c in complexity_range:\n",
    "            filename = f'{foldername}/GARCH_example_N{nTrain}_E{nEpochs}_c{c}_t{t}.h5'\n",
    "            filename_grad = f'{grad_foldername}/GARCH_example_N{nTrain}_E{nEpochs}_c{c}_t{t}.h5'\n",
    "            std_models[t][c] = load_model(filename)\n",
    "            grad_models[t][c] = load_model(filename_grad)\n",
    "    return std_models, grad_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1629834055888,
     "user": {
      "displayName": "Chris Mcdonagh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhF-UpNVeUXFvpwslXd-aD8GJZPFHZo2-yZhUuE-A=s64",
      "userId": "16046156616630541706"
     },
     "user_tz": -60
    },
    "id": "YU4ne-OYZRET"
   },
   "outputs": [],
   "source": [
    "def gen_preds(train_data, test_data, std_models, grad_models):\n",
    "    \n",
    "    simInData_train_latent = train_data['simInData_latent']\n",
    "    simInData_train_vol = train_data['simInData_vol']\n",
    "    simInData_train_ret = train_data['simInData_ret']\n",
    "\n",
    "    simInData_test_latent = test_data['simInData_latent']\n",
    "    simInData_test_vol = test_data['simInData_vol']\n",
    "    simInData_test_ret = test_data['simInData_ret']\n",
    "\n",
    "    simOutData_train = train_data['simOutData'] # Only used for constucting DFs and t0 population\n",
    "    simOutData_test = test_data['simOutData'] # Only used for constucting DFs and t0 population\n",
    "\n",
    "    N_train = len(simInData_train_latent)\n",
    "    N_test = len(simInData_test_latent)\n",
    "\n",
    "    resids_train = simInData_train_ret.iloc[:, 0] - simInData_train_latent.iloc[:, 0]\n",
    "    resids_test = simInData_test_ret.iloc[:, 0] - simInData_test_latent.iloc[:, 0]\n",
    "\n",
    "    std_preds_train = {}\n",
    "    std_preds_test = {}\n",
    "    \n",
    "    grad_preds_train = {}\n",
    "    grad_preds_test = {}\n",
    "    for c in complexity_range:\n",
    "        std_preds_train[c] = pd.DataFrame(index=simInData_train_latent.index, columns=forecast_window)\n",
    "        std_preds_test[c] = pd.DataFrame(index=simInData_test_latent.index, columns=forecast_window)\n",
    "        grad_preds_train[c] = pd.DataFrame(index=simInData_train_latent.index, columns=forecast_window)\n",
    "        grad_preds_test[c] = pd.DataFrame(index=simInData_test_latent.index, columns=forecast_window)\n",
    "\n",
    "    for t in forecast_window:\n",
    "        for c in complexity_range:\n",
    "            \n",
    "            crtModel_std = std_models[t][c]\n",
    "            crtModel_grad = grad_models[t][c]\n",
    "\n",
    "            if t == 1:\n",
    "                std_preds_train[c].loc[:, t] = crtModel_std.predict([simInData_train_latent.values, \n",
    "                                                    simInData_train_vol.values, resids_train.values])[0].flatten()\n",
    "                std_preds_test[c].loc[:, t] = crtModel_std.predict([simInData_test_latent.values, \n",
    "                                                    simInData_test_vol.values, resids_test.values])[0].flatten()\n",
    "                \n",
    "                grad_preds_train[c].loc[:, t] = crtModel_grad.predict([simInData_train_latent.values, \n",
    "                                                    simInData_train_vol.values, resids_train.values])[0].flatten()\n",
    "                grad_preds_test[c].loc[:, t] = crtModel_grad.predict([simInData_test_latent.values, \n",
    "                                                    simInData_test_vol.values, resids_test.values])[0].flatten()\n",
    "            else:\n",
    "                std_preds_train[c].loc[:, t] = crtModel_std.predict([simInData_train_latent.values, \n",
    "                                                    std_preds_train[c].loc[:, t-1].values])[0].flatten()\n",
    "                std_preds_test[c].loc[:, t] = crtModel_std.predict([simInData_test_latent.values, \n",
    "                                                    std_preds_test[c].loc[:, t-1].values])[0].flatten()\n",
    "                \n",
    "                grad_preds_train[c].loc[:, t] = crtModel_grad.predict([simInData_train_latent.values, \n",
    "                                                    grad_preds_train[c].loc[:, t-1].values])[0].flatten()\n",
    "                grad_preds_test[c].loc[:, t] = crtModel_grad.predict([simInData_test_latent.values, \n",
    "                                                    grad_preds_test[c].loc[:, t-1].values])[0].flatten()\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return std_preds_train, std_preds_test, grad_preds_train, grad_preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1629834055892,
     "user": {
      "displayName": "Chris Mcdonagh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhF-UpNVeUXFvpwslXd-aD8GJZPFHZo2-yZhUuE-A=s64",
      "userId": "16046156616630541706"
     },
     "user_tz": -60
    },
    "id": "evTGRWEuZUJU"
   },
   "outputs": [],
   "source": [
    "# Evaulation metrics\n",
    "def rmse(pred, true):\n",
    "    return np.sqrt(((pred.values - true.values)**2).mean().mean())\n",
    "\n",
    "def corr(pred, true):\n",
    "    return stats.pearsonr(pred.values.flatten(),true.values.flatten())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1629834055893,
     "user": {
      "displayName": "Chris Mcdonagh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhF-UpNVeUXFvpwslXd-aD8GJZPFHZo2-yZhUuE-A=s64",
      "userId": "16046156616630541706"
     },
     "user_tz": -60
    },
    "id": "e1axkYocZgNt"
   },
   "outputs": [],
   "source": [
    "def evaluate(train_data, test_data, std_preds_train, std_preds_test, grad_preds_train, grad_preds_test):\n",
    "    \n",
    "    simOutData_train = train_data['simOutData'] \n",
    "    simOutData_test = test_data['simOutData']\n",
    "    \n",
    "    std_train_res = defaultdict(dict)\n",
    "    std_test_res = defaultdict(dict)\n",
    "    grad_train_res = defaultdict(dict)\n",
    "    grad_test_res = defaultdict(dict)\n",
    "    \n",
    "    for c in complexity_range:\n",
    "        std_train_res[c]['rmse'] = rmse(std_preds_train[c], simOutData_train)\n",
    "        std_train_res[c]['corr'] = corr(std_preds_train[c], simOutData_train)\n",
    "        \n",
    "        std_test_res[c]['rmse'] = rmse(std_preds_test[c], simOutData_test)\n",
    "        std_test_res[c]['corr'] = corr(std_preds_test[c], simOutData_test)\n",
    "        \n",
    "        grad_train_res[c]['rmse'] = rmse(grad_preds_train[c], simOutData_train)\n",
    "        grad_train_res[c]['corr'] = corr(grad_preds_train[c], simOutData_train)\n",
    "        \n",
    "        grad_test_res[c]['rmse'] = rmse(grad_preds_test[c], simOutData_test)\n",
    "        grad_test_res[c]['corr'] = corr(grad_preds_test[c], simOutData_test)\n",
    "    \n",
    "    \n",
    "    return std_train_res, std_test_res, grad_train_res, grad_test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1629834055894,
     "user": {
      "displayName": "Chris Mcdonagh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhF-UpNVeUXFvpwslXd-aD8GJZPFHZo2-yZhUuE-A=s64",
      "userId": "16046156616630541706"
     },
     "user_tz": -60
    },
    "id": "LeRU_C6GZhiN"
   },
   "outputs": [],
   "source": [
    "def pipe(curr_data, test_data, N_train, foldername, idx):\n",
    "    \n",
    "    print(f'Beginning iteration {idx+1}')\n",
    "    \n",
    "    # Generate seperate foldername for each split to avoid confusion\n",
    "    foldername = foldername + f'_ntrain{N_train}_{idx}'\n",
    "    \n",
    "    # Build models\n",
    "    models_std, models_grad = build_models()\n",
    "    \n",
    "    print(f'Training models for dataset {idx+1}')\n",
    "    t0 = time1()\n",
    "    # Train models and save\n",
    "    train_models(curr_data, models_std, models_grad, foldername)\n",
    "    print(f'Trained models for dataset {idx+1} in {time1() - t0:.02f}s')\n",
    "     \n",
    "    print(f'Loading models for dataset {idx+1}')\n",
    "    # Load models from file\n",
    "    std_models, grad_models = load_models(N_train, foldername)\n",
    "    \n",
    "    print(f'Generating predictions for dataset {idx+1}')\n",
    "    # Generate predictions\n",
    "    std_preds_train, std_preds_test, grad_preds_train, grad_preds_test = gen_preds(curr_data, \n",
    "                                                test_data, std_models, grad_models)\n",
    "\n",
    "    std_train_res, std_test_res, grad_train_res, grad_test_res = evaluate(curr_data, test_data, \n",
    "                            std_preds_train, std_preds_test, grad_preds_train, grad_preds_test)\n",
    "    \n",
    "    return std_train_res, std_test_res, grad_train_res, grad_test_res, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1629834055897,
     "user": {
      "displayName": "Chris Mcdonagh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhF-UpNVeUXFvpwslXd-aD8GJZPFHZo2-yZhUuE-A=s64",
      "userId": "16046156616630541706"
     },
     "user_tz": -60
    },
    "id": "Yivz4RQZZjAd"
   },
   "outputs": [],
   "source": [
    "def run(N_train, no_ds, train_data, test_data, foldername='saved_models_cmplx_tests'):\n",
    "    \n",
    "    N_test = len(test_data['simInData_latent'])\n",
    "\n",
    "    print(f'Beginning process with:')\n",
    "    print(f'-> Epochs: {nEpochs}')\n",
    "    print(f'-> Complexity range: {complexity_range}')\n",
    "    print(f'-> {N_train} training samples')\n",
    "    print(f\"-> {N_test} test samples\")\n",
    "    print(f'-> Averging {no_ds} datasets')\n",
    "    \n",
    "    # Generate dataframes to store final results in\n",
    "    std_train_rmse = pd.DataFrame(index=complexity_range, columns=range(no_ds))\n",
    "    std_test_rmse = pd.DataFrame(index=complexity_range, columns=range(no_ds))\n",
    "    grad_train_rmse = pd.DataFrame(index=complexity_range, columns=range(no_ds))\n",
    "    grad_test_rmse = pd.DataFrame(index=complexity_range, columns=range(no_ds))\n",
    "    \n",
    "    std_train_corr = pd.DataFrame(index=complexity_range, columns=range(no_ds))\n",
    "    std_test_corr = pd.DataFrame(index=complexity_range, columns=range(no_ds))\n",
    "    grad_train_corr = pd.DataFrame(index=complexity_range, columns=range(no_ds))\n",
    "    grad_test_corr = pd.DataFrame(index=complexity_range, columns=range(no_ds))\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        future_to_res = {executor.submit(pipe, curr_data, test_data, \n",
    "                                        N_train, foldername, i): i \\\n",
    "                         for i, curr_data in train_data.items()}\n",
    "        for i in concurrent.futures.as_completed(future_to_res):\n",
    "            std_train_res, std_test_res, grad_train_res, grad_test_res, idx = i.result()\n",
    "            \n",
    "            for c in complexity_range:\n",
    "                std_train_rmse.loc[c, idx] = std_train_res[c]['rmse']\n",
    "                std_train_corr.loc[c, idx] = std_train_res[c]['corr']\n",
    "                \n",
    "                std_test_rmse.loc[c, idx] = std_test_res[c]['rmse']\n",
    "                std_test_corr.loc[c, idx] = std_test_res[c]['corr']\n",
    "            \n",
    "                grad_train_rmse.loc[c, idx] = grad_train_res[c]['rmse']\n",
    "                grad_train_corr.loc[c, idx] = grad_train_res[c]['corr']\n",
    "                \n",
    "                grad_test_rmse.loc[c, idx] = grad_test_res[c]['rmse']\n",
    "                grad_test_corr.loc[c, idx] = grad_test_res[c]['corr']\n",
    "        \n",
    "        print(f'Completed iteration {int(idx)+1} of {no_ds}')\n",
    "        \n",
    "    # Take averages/stds\n",
    "    std_train_rmse['mean'] = std_train_rmse.mean(axis=1)\n",
    "    std_train_rmse['std'] = std_train_rmse.std(axis=1)\n",
    "    \n",
    "    std_test_rmse['mean'] = std_test_rmse.mean(axis=1)\n",
    "    std_test_rmse['std'] = std_test_rmse.std(axis=1)\n",
    "    \n",
    "    std_train_corr['mean'] = std_train_corr.mean(axis=1)\n",
    "    std_train_corr['std'] = std_train_corr.std(axis=1)\n",
    "    \n",
    "    std_test_corr['mean'] = std_test_corr.mean(axis=1)\n",
    "    std_test_corr['std'] = std_test_corr.std(axis=1)\n",
    "    \n",
    "    grad_train_rmse['mean'] = grad_train_rmse.mean(axis=1)\n",
    "    grad_train_rmse['std'] = grad_train_rmse.std(axis=1)\n",
    "    \n",
    "    grad_test_rmse['mean'] = grad_test_rmse.mean(axis=1)\n",
    "    grad_test_rmse['std'] = grad_test_rmse.std(axis=1)\n",
    "    \n",
    "    grad_train_corr['mean'] = grad_train_corr.mean(axis=1)\n",
    "    grad_train_corr['std'] = grad_train_corr.std(axis=1)\n",
    "    \n",
    "    grad_test_corr['mean'] = grad_test_corr.mean(axis=1)\n",
    "    grad_test_corr['std'] = grad_test_corr.std(axis=1)\n",
    "    \n",
    "    c_time = datetime.now().strftime(\"%Y-%m-%d %H-%M\")\n",
    "    base_string = f'_ntrain{N_train}_ntest{N_test}_nEpoch{nEpochs}_Nruns{no_ds}_{c_time}.csv'\n",
    "    std_filename_train_rmse = f'std_train_rmse'+base_string\n",
    "    std_filename_train_corr = f'std_train_corr'+base_string\n",
    "    \n",
    "    std_filename_test_rmse = f'std_test_rmse'+base_string\n",
    "    std_filename_test_corr = f'std_test_corr'+base_string\n",
    "    \n",
    "    grad_filename_train_rmse = f'grad_train_rmse'+base_string\n",
    "    grad_filename_train_corr = f'grad_train_corr'+base_string\n",
    "    \n",
    "    grad_filename_test_rmse = f'grad_test_rmse'+base_string\n",
    "    grad_filename_test_corr = f'grad_test_corr'+base_string\n",
    "    \n",
    "    res_folder = f'complexity_test_results_ntrain{N_train}_{c_time}'\n",
    "    os.mkdir(res_folder)\n",
    "    std_train_rmse.to_csv(res_folder+'/'+std_filename_train_rmse)\n",
    "    std_train_corr.to_csv(res_folder+'/'+std_filename_train_corr)\n",
    "    std_test_rmse.to_csv(res_folder+'/'+std_filename_test_rmse)\n",
    "    std_test_corr.to_csv(res_folder+'/'+std_filename_test_corr)\n",
    "    \n",
    "    grad_train_rmse.to_csv(res_folder+'/'+grad_filename_train_rmse)\n",
    "    grad_train_corr.to_csv(res_folder+'/'+grad_filename_train_corr)\n",
    "    grad_test_rmse.to_csv(res_folder+'/'+grad_filename_test_rmse)\n",
    "    grad_test_corr.to_csv(res_folder+'/'+grad_filename_test_corr)\n",
    "    \n",
    "    #Zip up results so they can be downloaded.\n",
    "    #subprocess.call([\"zip\", \"-r\", f\"/content/{res_folder}.zip\", f\"/content/{res_folder}\"])\n",
    "    #files.download(f\"/content/{res_folder}.zip\")\n",
    "\n",
    "    print(f'Completed. Saved results to folder {res_folder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1629834055898,
     "user": {
      "displayName": "Chris Mcdonagh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhF-UpNVeUXFvpwslXd-aD8GJZPFHZo2-yZhUuE-A=s64",
      "userId": "16046156616630541706"
     },
     "user_tz": -60
    },
    "id": "oPm6EL2rUFx7"
   },
   "outputs": [],
   "source": [
    "def sub_sample_data(n_train, no_runs, all_train_data):\n",
    "    \n",
    "    train_data = defaultdict(dict) \n",
    "    for ds in range(no_runs):\n",
    "        np.random.seed(ds)\n",
    "        perm = np.random.permutation(n_train)\n",
    "        train_data[ds]['simInData_latent'] = all_train_data['simInData_latent'].iloc[perm, :].reset_index(drop=True)\n",
    "        train_data[ds]['simInData_ret'] = all_train_data['simInData_ret'].iloc[perm].reset_index(drop=True)\n",
    "        train_data[ds]['simInData_vol'] = all_train_data['simInData_vol'].iloc[perm].reset_index(drop=True)\n",
    "        train_data[ds]['simOutData'] = all_train_data['simOutData'].iloc[perm, :].reset_index(drop=True)\n",
    "        train_data[ds]['simOutData'].columns = forecast_window\n",
    "        train_data[ds]['simOutData_grad'] = all_train_data['simOutData_grad'].iloc[perm, :].reset_index(drop=True)\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 349,
     "status": "ok",
     "timestamp": 1629834056229,
     "user": {
      "displayName": "Chris Mcdonagh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhF-UpNVeUXFvpwslXd-aD8GJZPFHZo2-yZhUuE-A=s64",
      "userId": "16046156616630541706"
     },
     "user_tz": -60
    },
    "id": "gdXx_7sxiakd"
   },
   "outputs": [],
   "source": [
    "def lr_time_based_decay(epoch, lr):\n",
    "    return lr * 1 / (1 + decay * nEpochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2472,
     "status": "ok",
     "timestamp": 1629834058696,
     "user": {
      "displayName": "Chris Mcdonagh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhF-UpNVeUXFvpwslXd-aD8GJZPFHZo2-yZhUuE-A=s64",
      "userId": "16046156616630541706"
     },
     "user_tz": -60
    },
    "id": "5aZID6qqTerq",
    "outputId": "ee31cdb0-2f62-420e-9c7d-90227c9b3a13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read in dataset.\n"
     ]
    }
   ],
   "source": [
    "# Location of 4D data, generated by function presented in Chaper 3, Section 3.3.1\n",
    "data_folder = '4d_data'\n",
    "\n",
    "all_data_train = {}\n",
    "all_data_test = {}\n",
    "\n",
    "all_data_train['simInData_latent'] = pd.read_csv(f'{data_folder}/x_train_latent.csv', index_col=0)\n",
    "all_data_train['simInData_ret'] = pd.read_csv(f'{data_folder}/x_train_ret.csv', index_col=0)\n",
    "all_data_train['simInData_vol'] = pd.read_csv(f'{data_folder}/x_train_vol.csv', index_col=0)\n",
    "all_data_train['simOutData'] = pd.read_csv(f'{data_folder}/y_train.csv', index_col=0)\n",
    "all_data_train['simOutData_grad'] = pd.read_csv(f'{data_folder}/y_train_grad.csv', index_col=0, header=[0,1])\n",
    "\n",
    "all_data_test['simInData_latent'] = pd.read_csv(f'{data_folder}/x_test_latent.csv', index_col=0)\n",
    "all_data_test['simInData_ret'] = pd.read_csv(f'{data_folder}/x_test_ret.csv', index_col=0)\n",
    "all_data_test['simInData_vol'] = pd.read_csv(f'{data_folder}/x_test_vol.csv', index_col=0)\n",
    "all_data_test['simOutData'] = pd.read_csv(f'{data_folder}/y_test.csv', index_col=0)\n",
    "print(f'Successfully read in dataset.')\n",
    "# No need to read in test gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1629834058697,
     "user": {
      "displayName": "Chris Mcdonagh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhF-UpNVeUXFvpwslXd-aD8GJZPFHZo2-yZhUuE-A=s64",
      "userId": "16046156616630541706"
     },
     "user_tz": -60
    },
    "id": "pL-25tYT_2Td"
   },
   "outputs": [],
   "source": [
    "# Number of random draws from the training data pool to use\n",
    "n_runs = 5\n",
    "nTrain = 100\n",
    "\n",
    "train_data_ss_n100 = sub_sample_data(nTrain, n_runs, all_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1629834058698,
     "user": {
      "displayName": "Chris Mcdonagh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhF-UpNVeUXFvpwslXd-aD8GJZPFHZo2-yZhUuE-A=s64",
      "userId": "16046156616630541706"
     },
     "user_tz": -60
    },
    "id": "JU3mS5hNpUOB"
   },
   "outputs": [],
   "source": [
    "# Folder to save models to\n",
    "foldername = '/saved_models_nTrain100/saved_models_cmplx_tests'\n",
    "\n",
    "nEpochs = 300\n",
    "\n",
    "initial_learning_rate = 0.01\n",
    "decay = initial_learning_rate / nEpochs\n",
    "\n",
    "# Range of model complexities to experiment with\n",
    "complexity_range = [34, 38, 42, 46, 50, 54, 58, 62, 66, 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 7619292,
     "status": "ok",
     "timestamp": 1629841677983,
     "user": {
      "displayName": "Chris Mcdonagh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhF-UpNVeUXFvpwslXd-aD8GJZPFHZo2-yZhUuE-A=s64",
      "userId": "16046156616630541706"
     },
     "user_tz": -60
    },
    "id": "7gNXP6y-cUeU",
    "outputId": "8901aee3-f098-431c-ec9f-3290fb4de234"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning process with:\n",
      "-> Epochs: 300\n",
      "-> Complexity range: [34, 38, 42, 46, 50, 54, 58, 62, 66, 70]\n",
      "-> 100 training samples\n",
      "-> 1000 test samples\n",
      "-> Averging 5 datasets\n",
      "Beginning iteration 1\n",
      "Beginning iteration 2Beginning iteration 3\n",
      "\n",
      "Beginning iteration 4Beginning iteration 5\n",
      "\n",
      "Training models for dataset 3\n",
      "Training models for dataset 5\n",
      "Training models for dataset 4\n",
      "Training models for dataset 1\n",
      "Training models for dataset 2\n",
      "Completed timestep 1 in 1344.73s...\n",
      "Completed timestep 1 in 1340.40s...\n",
      "Completed timestep 1 in 1345.78s...\n",
      "Completed timestep 1 in 1356.09s...\n",
      "Completed timestep 1 in 1356.64s...\n",
      "Completed timestep 2 in 1270.68s...\n",
      "Completed timestep 2 in 1264.45s...\n",
      "Completed timestep 2 in 1275.97s...\n",
      "Completed timestep 2 in 1276.37s...\n",
      "Completed timestep 2 in 1260.63s...\n",
      "Completed timestep 3 in 1422.23s...\n",
      "Completed timestep 3 in 1482.01s...\n",
      "Completed timestep 3 in 1491.14s...\n",
      "Completed timestep 3 in 1500.18s...\n",
      "Completed timestep 3 in 1515.58s...\n",
      "Completed timestep 4 in 1347.60s...\n",
      "Completed timestep 4 in 1534.00s...\n",
      "Completed timestep 4 in 1547.31s...\n",
      "Completed timestep 4 in 1560.02s...\n",
      "Completed timestep 4 in 1544.86s...\n",
      "Completed timestep 5 in 1347.48s...\n",
      "Trained models for dataset 5 in 6732.78s\n",
      "Loading models for dataset 5\n",
      "Generating predictions for dataset 5\n",
      "Completed timestep 5 in 1367.94s...\n",
      "Trained models for dataset 2 in 7001.24s\n",
      "Loading models for dataset 2\n",
      "Completed timestep 5 in 1416.73s...\n",
      "Trained models for dataset 3 in 7097.80s\n",
      "Loading models for dataset 3\n",
      "Completed timestep 5 in 1430.61s...\n",
      "Trained models for dataset 1 in 7098.14s\n",
      "Loading models for dataset 1\n",
      "Completed timestep 5 in 1434.83s...\n",
      "Trained models for dataset 4 in 7104.50s\n",
      "Loading models for dataset 4\n",
      "Generating predictions for dataset 2\n",
      "Generating predictions for dataset 3\n",
      "Generating predictions for dataset 4\n",
      "Generating predictions for dataset 1\n",
      "Completed iteration 3 of 5\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_a1bdbe7c-c218-414c-9aff-e3c506210725\", \"complexity_test_results_ntrain100_2021-08-24 21-47.zip\", 8923)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed. Saved results to folder complexity_test_results_ntrain100_2021-08-24 21-47\n"
     ]
    }
   ],
   "source": [
    "# Running this cell runs experiments\n",
    "#run(nTrain, n_runs, train_data_ss_n100, all_data_test, foldername=foldername)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyMwwg3keV6sCqxiz9puoaVO",
   "collapsed_sections": [],
   "mount_file_id": "1tpV4AhlpmBOvW7alPClEs-keSE5UIFoU",
   "name": "GARCH complexity test: nTrain 100.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
