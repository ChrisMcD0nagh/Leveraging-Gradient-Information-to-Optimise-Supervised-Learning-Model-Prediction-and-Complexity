{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D surrogate training code\n",
    "\n",
    "This notebook provides an example of the code used to train and evaluate the accuracy of surrogate models for the 2D toy problem discussed in Chapter 3 Section 3.2. \n",
    "\n",
    "Original experiments were run in Google Colab using TPUs. \n",
    "\n",
    "This code is replicated with varying training sizes to produce the full result set. Full code and models are available [here](https://drive.google.com/drive/folders/1J7srZbZPS6UhE43GFXP3Gkd3TmEvT-6f)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8Ezc5LO_QDV"
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda, dot, concatenate, PReLU, Dropout, advanced_activations\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import adam_v2  # Note use Adam if adam_v2.Adam throws version error \n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import concurrent.futures\n",
    "from time import time\n",
    "import gc\n",
    "from scipy import stats\n",
    "from datetime import *\n",
    "from time import time as time1\n",
    "import os\n",
    "import subprocess\n",
    "#from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYDJk9vd_mCI"
   },
   "outputs": [],
   "source": [
    "# Suppress retracing  and auograph error\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMvXCMPB_wL8"
   },
   "outputs": [],
   "source": [
    "# Parameters for 2D toy problem\n",
    "nInputDim = 2\n",
    "nOutputDim = 1\n",
    "T = 10 # Time steps for dynamic simulator\n",
    "yearIndSet = np.arange(T)\n",
    "base_hidden_size = 4 # Base number of nodes for each layer in surrogate model\n",
    "nBatchSize = 10 # Batch size for training Keras neural network models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "\n",
    "Initial (t0) surrogate takes only latent parameters $\\theta$ as an input while subsequent surrogates take latent parameters and previous output as described in Chapter 2 Section 2.1.1.\n",
    "\n",
    "Surrogate models use architecture with 4 hidden layers, each with $\\eta$ * 4 nodes where $\\eta$ refers to model complexity as described in Chapter 3 Section 3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bSZTZ6lAkrU1"
   },
   "outputs": [],
   "source": [
    "def make_model_t0(complexity=1, lr=0.001, g_w=0.5):\n",
    "    \"\"\"\"Make the initial surrogate that does not take prior outputs as inputs.\n",
    "    The weight assigned to the loss function that fits gradients can be set with g_w.\n",
    "    Model complexity integer and determines the number of nodes in each \n",
    "    hidden layer.\n",
    "    \"\"\"\n",
    "    theta = Input(shape=nInputDim)\n",
    "    h1 = Dense(complexity * base_hidden_size, activation=\"tanh\")(theta)\n",
    "    h2 = Dense(complexity * base_hidden_size, activation=\"tanh\")(h1)\n",
    "    h3 = Dense(complexity * base_hidden_size, activation=\"tanh\")(h2)\n",
    "    h4 = Dense(complexity * base_hidden_size, activation=\"tanh\")(h3)\n",
    "    out = Dense(nOutputDim, activation='linear')(h4)\n",
    "    \n",
    "    grad = Lambda(lambda x: K.gradients(x[0], [x[1]])[0], output_shape=nInputDim)([out, theta])\n",
    "    model = Model(inputs=[theta], outputs=[out, grad])\n",
    "    opt = adam_v2.Adam(learning_rate=lr)\n",
    "    model.compile(loss=['mse', 'mse'], optimizer=opt, metrics=['accuracy'], loss_weights=[1-g_w, g_w])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def make_model_sub(complexity=1, lr=0.001, g_w=0.5):\n",
    "    \"\"\"\"Make the subsequent surrogates that do not take prior outputs as inputs.\n",
    "    The weight assigned to the loss function that fits gradients can be set with g_w.\n",
    "    Model complexity integer and determines the number of nodes in each \n",
    "    hidden layer.\n",
    "    \"\"\"\n",
    "    theta = Input(shape=nInputDim)\n",
    "    prev_output = Input(shape=nOutputDim)\n",
    "    concat = concatenate([theta, prev_output])\n",
    "    h1 = Dense(complexity * base_hidden_size, activation=\"tanh\")(concat)\n",
    "    h2 = Dense(complexity * base_hidden_size, activation=\"tanh\")(h1)\n",
    "    h3 = Dense(complexity * base_hidden_size, activation=\"tanh\")(h2)\n",
    "    h4 = Dense(complexity * base_hidden_size, activation=\"tanh\")(h3)\n",
    "    out = Dense(nOutputDim, activation='linear')(h4)\n",
    "    \n",
    "    grad = Lambda(lambda x: K.gradients(x[0], [x[1]])[0], output_shape=nInputDim)([out, theta])\n",
    "    model = Model(inputs=[theta, prev_output], outputs=[out, grad])\n",
    "    opt = adam_v2.Adam(learning_rate=lr)\n",
    "    model.compile(loss=['mse', 'mse'], optimizer=opt, metrics=['accuracy'], loss_weights=[1-g_w, g_w])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bSZTZ6lAkrU1"
   },
   "outputs": [],
   "source": [
    "def build_models():\n",
    "    \"\"\" Store models in nested dictionary of the form [time_step][complexity].\n",
    "    \"\"\"\n",
    "    \n",
    "    models_std = defaultdict(dict)\n",
    "    models_grad = defaultdict(dict)\n",
    "    \n",
    "    for i in yearIndSet:\n",
    "        for m in complexity_range:\n",
    "        \n",
    "            if i == 0:\n",
    "                models_std[str(i)][str(m)] = make_model_t0(complexity=m, g_w=0)\n",
    "                models_grad[str(i)][str(m)] = make_model_t0(complexity=m)\n",
    "            else:\n",
    "                models_std[str(i)][str(m)] = make_model_sub(complexity=m, g_w=0)\n",
    "                models_grad[str(i)][str(m)] = make_model_sub(complexity=m)\n",
    "         \n",
    "        \n",
    "    return models_std, models_grad\n",
    "\n",
    "def fit_model(model, x, prev_output, y, y_grad, epochs, yearIdx, complexity, N, foldername):\n",
    "    \"\"\" Fits the NN models.\n",
    "    Note that when yearIdx=0, prev_output takes value -1 which is incorrect (should be no prev_output), \n",
    "    but it is not used.\n",
    "    \"\"\"\n",
    "    if yearIdx == 0:\n",
    "        model.fit([x], [y, y_grad], batch_size=nBatchSize, epochs=epochs, verbose=0, \n",
    "        use_multiprocessing=True, callbacks=[LearningRateScheduler(lr_time_based_decay)])\n",
    "    else:\n",
    "        model.fit([x, prev_output], [y, y_grad], batch_size=nBatchSize, epochs=epochs, verbose=0,\n",
    "        use_multiprocessing=True, callbacks=[LearningRateScheduler(lr_time_based_decay)])\n",
    "    \n",
    "    filename = f'{foldername}/2D_example_N{N}_E{epochs}_t{yearIdx}_c{complexity}.h5'\n",
    "    model.save(filepath=filename)\n",
    "\n",
    "def train_models(curr_data, nEpoch, models_std, models_grad, foldername):\n",
    "    \n",
    "    simInData= curr_data['simInData']\n",
    "    simOutData = curr_data['simOutData']\n",
    "    simOutData_grad = curr_data['simOutData_grad']\n",
    "    \n",
    "    no_models = len(complexity_range)\n",
    "    no_timesteps = len(yearIndSet)\n",
    "    N = len(simInData)\n",
    "#     print(f'Starting training of {no_models} models with {no_timesteps} time steps ({no_models * no_timesteps * 2} total models.) ')\n",
    "#     print(f'-> N = {N}')\n",
    "#     print(f'-> Epochs = {nEpoch}')\n",
    "\n",
    "    foldername_grad = foldername+'_grad'\n",
    "\n",
    "    t0 = time1()\n",
    "    t_curr = time1()\n",
    "    for yearIdx in yearIndSet:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=no_models) as executor:\n",
    "            future1 = {executor.submit(fit_model, \n",
    "                                      m, \n",
    "                                      simInData.values, simOutData.iloc[:, yearIdx-1].values.flatten(), \n",
    "                                      simOutData.iloc[:, yearIdx].values, \n",
    "                                      simOutData_grad.loc[:, str(yearIdx)].values, \n",
    "                                      nEpoch, \n",
    "                                      yearIdx, \n",
    "                                      complexity, \n",
    "                                      N, \n",
    "                                      foldername) \n",
    "                    for complexity, m in models_std[str(yearIdx)].items()}\n",
    "            \n",
    "            future2 = {executor.submit(fit_model, \n",
    "                                      m, \n",
    "                                      simInData.values, simOutData.iloc[:, yearIdx-1].values.flatten(), \n",
    "                                      simOutData.iloc[:, yearIdx].values, \n",
    "                                      simOutData_grad.loc[:, str(yearIdx)].values, \n",
    "                                      nEpoch, \n",
    "                                      yearIdx, \n",
    "                                      complexity, \n",
    "                                      N, \n",
    "                                      foldername_grad) \n",
    "                    for complexity, m in models_grad[str(yearIdx)].items()}\n",
    "        print(f'Completed timestep {yearIdx+1} in {time1() - t_curr:.02f}s...')\n",
    "        t_curr = time1()\n",
    "#     print(f'Completed in {(time1() - t0) / 60:.02f} mins.')\n",
    "\n",
    "\n",
    "def load_models(epochs, N, foldername):\n",
    "    std_models = defaultdict(dict)\n",
    "    grad_models = defaultdict(dict)\n",
    "    \n",
    "    grad_foldername = foldername + '_grad' \n",
    "    \n",
    "    for yearIdx in yearIndSet:\n",
    "        for c in complexity_range:\n",
    "            filename = f'{foldername}/2D_example_N{N}_E{epochs}_t{yearIdx}_c{c}.h5'\n",
    "            filename_grad = f'{grad_foldername}/2D_example_N{N}_E{epochs}_t{yearIdx}_c{c}.h5'\n",
    "            std_models[yearIdx][c] = load_model(filename)\n",
    "            grad_models[yearIdx][c] = load_model(filename_grad)\n",
    "    return std_models, grad_models\n",
    "\n",
    "def gen_preds(train_data, test_data, std_models, grad_models):\n",
    "    \n",
    "    simInData_train = train_data['simInData']\n",
    "    simInData_test = test_data['simInData']\n",
    "    simOutData_train = train_data['simOutData'] # Only used for constucting DFs\n",
    "    simOutData_test = test_data['simOutData'] # Only used for constucting DFs\n",
    "\n",
    "    std_preds_train = {}\n",
    "    std_preds_test = {}\n",
    "    \n",
    "    grad_preds_train = {}\n",
    "    grad_preds_test = {}\n",
    "    for c in complexity_range:\n",
    "        std_preds_train[c] = pd.DataFrame(index=simOutData_train.index, columns=simOutData_train.columns)\n",
    "        std_preds_test[c] = pd.DataFrame(index=simOutData_test.index, columns=simOutData_test.columns)\n",
    "        grad_preds_train[c] = pd.DataFrame(index=simOutData_train.index, columns=simOutData_train.columns)\n",
    "        grad_preds_test[c] = pd.DataFrame(index=simOutData_test.index, columns=simOutData_test.columns)\n",
    "        \n",
    "    for yearIdx in yearIndSet:\n",
    "        for c in complexity_range:\n",
    "            crtModel_std = std_models[yearIdx][c]\n",
    "            crtModel_grad = grad_models[yearIdx][c]\n",
    "            \n",
    "#             print(f'T: {yearIdx}, c: {c}')\n",
    "            \n",
    "            if yearIdx == 0:\n",
    "                # Generate predictinos and assign data\n",
    "                std_preds_train[c].iloc[:, yearIdx] = crtModel_std.predict_on_batch(simInData_train.values)[0].flatten()\n",
    "                std_preds_test[c].iloc[:, yearIdx] = crtModel_std.predict_on_batch(simInData_test.values)[0].flatten()\n",
    "                grad_preds_train[c].iloc[:, yearIdx] = crtModel_grad.predict_on_batch(simInData_train.values)[0].flatten()\n",
    "                grad_preds_test[c].iloc[:, yearIdx] = crtModel_grad.predict_on_batch(simInData_test.values)[0].flatten()\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                std_preds_train[c].iloc[:, yearIdx] = crtModel_std.predict_on_batch([simInData_train.values, \n",
    "                                                    std_preds_train[c].iloc[:, yearIdx-1]])[0].flatten()\n",
    "                std_preds_test[c].iloc[:, yearIdx] = crtModel_std.predict_on_batch([simInData_test.values, \n",
    "                                                    std_preds_test[c].iloc[:, yearIdx-1]])[0].flatten()\n",
    "                \n",
    "                grad_preds_train[c].iloc[:, yearIdx] = crtModel_grad.predict_on_batch([simInData_train.values, \n",
    "                                                        grad_preds_train[c].iloc[:, yearIdx-1]])[0].flatten()\n",
    "                grad_preds_test[c].iloc[:, yearIdx] = crtModel_grad.predict_on_batch([simInData_test.values, \n",
    "                                                        grad_preds_test[c].iloc[:, yearIdx-1]])[0].flatten()\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    return std_preds_train, std_preds_test, grad_preds_train, grad_preds_test\n",
    "                \n",
    "\n",
    "# Evaulation metrics\n",
    "def rmse(pred, true):\n",
    "    return np.sqrt(((pred - true)**2).mean().mean())\n",
    "\n",
    "def corr(pred, true):\n",
    "    return stats.pearsonr(pred.values.flatten(),true.values.flatten())[0]\n",
    "\n",
    "def evaluate(train_data, test_data, std_preds_train, std_preds_test, grad_preds_train, grad_preds_test):\n",
    "    \n",
    "    simOutData_train = train_data['simOutData'] \n",
    "    simOutData_test = test_data['simOutData']\n",
    "    \n",
    "    std_train_res = defaultdict(dict)\n",
    "    std_test_res = defaultdict(dict)\n",
    "    grad_train_res = defaultdict(dict)\n",
    "    grad_test_res = defaultdict(dict)\n",
    "    \n",
    "    for c in complexity_range:\n",
    "        std_train_res[c]['rmse'] = rmse(std_preds_train[c], simOutData_train)\n",
    "        std_train_res[c]['corr'] = corr(std_preds_train[c], simOutData_train)\n",
    "        \n",
    "        std_test_res[c]['rmse'] = rmse(std_preds_test[c], simOutData_test)\n",
    "        std_test_res[c]['corr'] = corr(std_preds_test[c], simOutData_test)\n",
    "        \n",
    "        grad_train_res[c]['rmse'] = rmse(grad_preds_train[c], simOutData_train)\n",
    "        grad_train_res[c]['corr'] = corr(grad_preds_train[c], simOutData_train)\n",
    "        \n",
    "        grad_test_res[c]['rmse'] = rmse(grad_preds_test[c], simOutData_test)\n",
    "        grad_test_res[c]['corr'] = corr(grad_preds_test[c], simOutData_test)\n",
    "    \n",
    "    \n",
    "    return std_train_res, std_test_res, grad_train_res, grad_test_res\n",
    "\n",
    "def pipe(curr_data, test_data, nEpoch, N_train, foldername, idx):\n",
    "    \n",
    "    print(f'Beginning iteration {idx+1}')\n",
    "    \n",
    "    # Generate seperate foldername for each split to avoid confusion\n",
    "    foldername = foldername + f'_ntrain{N_train}_{idx}'\n",
    "    \n",
    "    # Build models\n",
    "    models_std, models_grad = build_models()\n",
    "    \n",
    "    print(f'Training models for dataset {idx+1}')\n",
    "    t0 = time1()\n",
    "    # Train models and save\n",
    "    train_models(curr_data, nEpoch, models_std, models_grad, foldername)\n",
    "    print(f'Trained models for dataset {idx+1} in {time1() - t0:.02f}s')\n",
    "    \n",
    "    print(f'Loading models for dataset {idx+1}')\n",
    "    # Load models from file\n",
    "    std_models, grad_models = load_models(nEpoch, N_train, foldername)\n",
    "    \n",
    "    print(f'Generating predictions for dataset {idx+1}')\n",
    "    # Generate predictions\n",
    "    std_preds_train, std_preds_test, grad_preds_train, grad_preds_test = gen_preds(curr_data, \n",
    "                                                test_data, std_models, grad_models)\n",
    "    # Evaluate predictions\n",
    "    std_train_res, std_test_res, grad_train_res, grad_test_res = evaluate(curr_data, test_data, \n",
    "                            std_preds_train, std_preds_test, grad_preds_train, grad_preds_test)\n",
    "    \n",
    "    return std_train_res, std_test_res, grad_train_res, grad_test_res, idx\n",
    "    \n",
    "    \n",
    "\n",
    "def run(N_train, nEpoch, no_ds, train_data, test_data, foldername='saved_models_cmplx_tests'):\n",
    "    \n",
    "    N_test = len(test_data['simInData'])\n",
    "\n",
    "    print(f'Beginning process with:')\n",
    "    print(f'-> Epochs: {nEpoch}')\n",
    "    print(f'-> Complexity range: {complexity_range}')\n",
    "    print(f'-> {N_train} training samples')\n",
    "    print(f\"-> {N_test} test samples\")\n",
    "    print(f'-> Averging {no_ds} datasets')\n",
    "    \n",
    "    # Generate dataframes to store final results in\n",
    "    std_train_rmse = pd.DataFrame(index=complexity_range, columns=range(no_ds))\n",
    "    std_test_rmse = pd.DataFrame(index=complexity_range, columns=range(no_ds))\n",
    "    grad_train_rmse = pd.DataFrame(index=complexity_range, columns=range(no_ds))\n",
    "    grad_test_rmse = pd.DataFrame(index=complexity_range, columns=range(no_ds))\n",
    "    \n",
    "    std_train_corr = pd.DataFrame(index=complexity_range, columns=range(no_ds))\n",
    "    std_test_corr = pd.DataFrame(index=complexity_range, columns=range(no_ds))\n",
    "    grad_train_corr = pd.DataFrame(index=complexity_range, columns=range(no_ds))\n",
    "    grad_test_corr = pd.DataFrame(index=complexity_range, columns=range(no_ds))\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        future_to_res = {executor.submit(pipe, curr_data, test_data, nEpoch, \n",
    "                                        N_train, foldername, i): i \\\n",
    "                         for i, curr_data in train_data.items()}\n",
    "        for i in concurrent.futures.as_completed(future_to_res):\n",
    "            std_train_res, std_test_res, grad_train_res, grad_test_res, idx = i.result()\n",
    "            \n",
    "            for c in complexity_range:\n",
    "                std_train_rmse.loc[c, idx] = std_train_res[c]['rmse']\n",
    "                std_train_corr.loc[c, idx] = std_train_res[c]['corr']\n",
    "                \n",
    "                std_test_rmse.loc[c, idx] = std_test_res[c]['rmse']\n",
    "                std_test_corr.loc[c, idx] = std_test_res[c]['corr']\n",
    "            \n",
    "                grad_train_rmse.loc[c, idx] = grad_train_res[c]['rmse']\n",
    "                grad_train_corr.loc[c, idx] = grad_train_res[c]['corr']\n",
    "                \n",
    "                grad_test_rmse.loc[c, idx] = grad_test_res[c]['rmse']\n",
    "                grad_test_corr.loc[c, idx] = grad_test_res[c]['corr']\n",
    "        \n",
    "        print(f'Completed iteration {int(idx)+1} of {no_ds}')\n",
    "        \n",
    "    # Take averages/stds\n",
    "    std_train_rmse['mean'] = std_train_rmse.mean(axis=1)\n",
    "    std_train_rmse['std'] = std_train_rmse.std(axis=1)\n",
    "    \n",
    "    std_test_rmse['mean'] = std_test_rmse.mean(axis=1)\n",
    "    std_test_rmse['std'] = std_test_rmse.std(axis=1)\n",
    "    \n",
    "    std_train_corr['mean'] = std_train_corr.mean(axis=1)\n",
    "    std_train_corr['std'] = std_train_corr.std(axis=1)\n",
    "    \n",
    "    std_test_corr['mean'] = std_test_corr.mean(axis=1)\n",
    "    std_test_corr['std'] = std_test_corr.std(axis=1)\n",
    "    \n",
    "    grad_train_rmse['mean'] = grad_train_rmse.mean(axis=1)\n",
    "    grad_train_rmse['std'] = grad_train_rmse.std(axis=1)\n",
    "    \n",
    "    grad_test_rmse['mean'] = grad_test_rmse.mean(axis=1)\n",
    "    grad_test_rmse['std'] = grad_test_rmse.std(axis=1)\n",
    "    \n",
    "    grad_train_corr['mean'] = grad_train_corr.mean(axis=1)\n",
    "    grad_train_corr['std'] = grad_train_corr.std(axis=1)\n",
    "    \n",
    "    grad_test_corr['mean'] = grad_test_corr.mean(axis=1)\n",
    "    grad_test_corr['std'] = grad_test_corr.std(axis=1)\n",
    "    \n",
    "    c_time = datetime.now().strftime(\"%Y-%m-%d %H-%M\")\n",
    "    base_string = f'_ntrain{N_train}_ntest{N_test}_nEpoch{nEpoch}_Nruns{no_ds}_{c_time}.csv'\n",
    "    std_filename_train_rmse = f'std_train_rmse'+base_string\n",
    "    std_filename_train_corr = f'std_train_corr'+base_string\n",
    "    \n",
    "    std_filename_test_rmse = f'std_test_rmse'+base_string\n",
    "    std_filename_test_corr = f'std_test_corr'+base_string\n",
    "    \n",
    "    grad_filename_train_rmse = f'grad_train_rmse'+base_string\n",
    "    grad_filename_train_corr = f'grad_train_corr'+base_string\n",
    "    \n",
    "    grad_filename_test_rmse = f'grad_test_rmse'+base_string\n",
    "    grad_filename_test_corr = f'grad_test_corr'+base_string\n",
    "    \n",
    "    res_folder = f'complexity_test_results_ntrain{N_train}_{c_time}'\n",
    "    os.mkdir(res_folder)\n",
    "    std_train_rmse.to_csv(res_folder+'/'+std_filename_train_rmse)\n",
    "    std_train_corr.to_csv(res_folder+'/'+std_filename_train_corr)\n",
    "    std_test_rmse.to_csv(res_folder+'/'+std_filename_test_rmse)\n",
    "    std_test_corr.to_csv(res_folder+'/'+std_filename_test_corr)\n",
    "    \n",
    "    grad_train_rmse.to_csv(res_folder+'/'+grad_filename_train_rmse)\n",
    "    grad_train_corr.to_csv(res_folder+'/'+grad_filename_train_corr)\n",
    "    grad_test_rmse.to_csv(res_folder+'/'+grad_filename_test_rmse)\n",
    "    grad_test_corr.to_csv(res_folder+'/'+grad_filename_test_corr)\n",
    "    \n",
    "    # Zip up results so they can be downloaded (commented out for example NB).\n",
    "#     subprocess.call([\"zip\", \"-r\", f\"/content/{res_folder}.zip\", f\"/content/{res_folder}\"])\n",
    "#     files.download(f\"/content/{res_folder}.zip\")\n",
    "\n",
    "    print(f'Completed. Saved results to folder {res_folder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oPm6EL2rUFx7"
   },
   "outputs": [],
   "source": [
    "def sub_sample_data(n_train, no_runs, all_train_data):\n",
    "    \n",
    "    train_data = defaultdict(dict) \n",
    "    for ds in range(no_runs):\n",
    "        np.random.seed(ds)\n",
    "        perm = np.random.permutation(n_train)\n",
    "        train_data[ds]['simInData'] = all_train_data['simInData'].iloc[perm].reset_index(drop=True)\n",
    "        train_data[ds]['simOutData'] = all_train_data['simOutData'].iloc[perm].reset_index(drop=True)\n",
    "        train_data[ds]['simOutData_grad'] = all_train_data['simOutData_grad'].iloc[perm].reset_index(drop=True)\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdXx_7sxiakd"
   },
   "outputs": [],
   "source": [
    "def lr_time_based_decay(epoch, lr):\n",
    "    return lr * 1 / (1 + decay * n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2272,
     "status": "ok",
     "timestamp": 1630503767152,
     "user": {
      "displayName": "Chris Mcdonagh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhF-UpNVeUXFvpwslXd-aD8GJZPFHZo2-yZhUuE-A=s64",
      "userId": "16046156616630541706"
     },
     "user_tz": -60
    },
    "id": "5aZID6qqTerq",
    "outputId": "cd3be339-2a9b-47d2-f22f-7cc17e9a6cbb"
   },
   "outputs": [],
   "source": [
    "# Location of 2D data, generated by function presented in Chaper 3, Section 3.2.1\n",
    "data_folder = '/content/drive/MyDrive/Colab Notebooks/rp/2D process/2D_data'\n",
    "\n",
    "all_data_train = {}\n",
    "all_data_test = {}\n",
    "\n",
    "all_data_train['simInData'] = pd.read_csv(f'{data_folder}/x_train.csv', index_col=0)\n",
    "all_data_train['simOutData'] = pd.read_csv(f'{data_folder}/y_train.csv', index_col=0)\n",
    "all_data_train['simOutData_grad'] = pd.read_csv(f'{data_folder}/y_train_grad.csv', index_col=0, header=[0,1])\n",
    "all_data_test['simInData'] = pd.read_csv(f'{data_folder}/x_test.csv', index_col=0)\n",
    "all_data_test['simOutData'] = pd.read_csv(f'{data_folder}/y_test.csv', index_col=0)\n",
    "print(f'Successfully read in dataset.')\n",
    "# No need to read in test gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pL-25tYT_2Td"
   },
   "outputs": [],
   "source": [
    "# Number of random draws from the training data pool to use\n",
    "n_runs = 10\n",
    "\n",
    "train_data_ss = sub_sample_data(50, n_runs, all_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVK1gNGMUfqg"
   },
   "outputs": [],
   "source": [
    "# Folder to save models to\n",
    "foldername = '/content/drive/MyDrive/2D_models/saved_models_nTrain50_lrd/saved_models_cmplx_tests'\n",
    "n_epoch = 300\n",
    "initial_learning_rate = 0.01\n",
    "decay = initial_learning_rate / n_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZQVkzrfcx8g"
   },
   "outputs": [],
   "source": [
    "# Range of model complexities to experiment with\n",
    "complexity_range = [2, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 659957,
     "status": "ok",
     "timestamp": 1630504427289,
     "user": {
      "displayName": "Chris Mcdonagh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhF-UpNVeUXFvpwslXd-aD8GJZPFHZo2-yZhUuE-A=s64",
      "userId": "16046156616630541706"
     },
     "user_tz": -60
    },
    "id": "7gNXP6y-cUeU",
    "outputId": "d90a28cd-9254-40f3-cbcb-9e238286110e"
   },
   "outputs": [],
   "source": [
    "# Running this cell runs experiments\n",
    "#run(50, n_epoch, n_runs, train_data_ss, all_data_test, foldername=foldername)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyMFiTAkzyvA2/NtFvMmC8vX",
   "collapsed_sections": [],
   "mount_file_id": "1NR6eYhjdXhtio-_u7mVHDNcEZ-2xpUeI",
   "name": "complexity_test_nTrain50.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
